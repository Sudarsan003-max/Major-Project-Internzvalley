# -*- coding: utf-8 -*-
"""Major_Project_internzvalley.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J-mhTQNMMdw8-DS9hKV4QKVtp4IpV7KG
"""

## Importing the necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Added for Google Colab
from google.colab import files
uploaded = files.upload()

### Load the dataset
data = pd.read_csv('heart-disease (1).csv')
print('Shape of the data is:', data.shape)

data.info()

pd.set_option('display.float_format', lambda x: '%.3f' % x)
data.describe().transpose()

data.head()

# Check for missing values
print(data.isnull().sum())

print(data.columns)

## Data Filtering
columns_to_drop = ['slope', 'ca']
existing_columns = [col for col in columns_to_drop if col in data.columns]
if existing_columns:
    data = data.drop(existing_columns, axis=1)
    print(f"Dropped columns: {existing_columns}")
else:
    print(f"Columns {columns_to_drop} not found in the DataFrame.")


data.shape

data.isnull

data.duplicated().sum()

# Visualize data distribution
sns.pairplot(data, hue='target')

plt.show()

# Handle missing values (if any)
data.fillna(method='ffill', inplace=True)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Default parameters
rf_default = RandomForestClassifier()

rf_default.fit(X_train, y_train)

# Predictions and evaluation
y_pred_default = rf_default.predict(X_test)

print("Confusion Matrix (Default Parameters):")

print(confusion_matrix(y_test, y_pred_default))

print("Classification Report (Default Parameters):")

print(classification_report(y_test, y_pred_default))

# Tuned parameters
rf_tuned = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

rf_tuned.fit(X_train, y_train)

# Predictions and evaluation
y_pred_tuned = rf_tuned.predict(X_test)

print("Confusion Matrix (Tuned Parameters):")

print(confusion_matrix(y_test, y_pred_tuned))

print("Classification Report (Tuned Parameters):")

print(classification_report(y_test, y_pred_tuned))

# Feature importance
importances = rf_tuned.feature_importances_

indices = np.argsort(importances)[::-1]

# Plot
plt.figure(figsize=(12, 8))

plt.title("Feature Importances")

plt.bar(range(X.shape[1]), importances[indices], align='center')

plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)

plt.show()

# Predictions and evaluation for default parameters
y_pred_default = rf_default.predict(X_test)

conf_matrix_default = confusion_matrix(y_test, y_pred_default)

class_report_default = classification_report(y_test, y_pred_default)

# Predictions and evaluation for tuned parameters
y_pred_tuned = rf_tuned.predict(X_test)

conf_matrix_tuned = confusion_matrix(y_test, y_pred_tuned)

class_report_tuned = classification_report(y_test, y_pred_tuned)

# Print evaluation results
print("Confusion Matrix (Default Parameters):")

print(conf_matrix_default)

print("\nClassification Report (Default Parameters):")

print(class_report_default)

print("\nConfusion Matrix (Tuned Parameters):")

print(conf_matrix_tuned)

print("\nClassification Report (Tuned Parameters):")

print(class_report_tuned)

# Feature importance
importances = rf_tuned.feature_importances_

indices = np.argsort(importances)[::-1]

# print feature ranking
print("Feature Ranking:")

for f in range(X.shape[1]):
    print(f"{f + 1}. Feature {X.columns[indices[f]]} ({importances[indices[f]]})")

### Summary of findings
print("\nImplications of the Findings:")

### Compare performance metrics
default_accuracy = (conf_matrix_default[0, 0] + conf_matrix_default[1, 1]) / np.sum(conf_matrix_default)

tuned_accuracy = (conf_matrix_tuned[0, 0] + conf_matrix_tuned[1, 1]) / np.sum(conf_matrix_tuned)

print(f"Accuracy (Default Parameters): {default_accuracy:.2f}")

print(f"Accuracy (Tuned Parameters): {tuned_accuracy:.2f}")

### Performance comparison
if tuned_accuracy > default_accuracy:
    print("\nThe tuned Random Forest model shows improved accuracy compared to the default model, suggesting that parameter tuning can significantly enhance model performance.")
else:
    print("\nThe tuned Random Forest model does not show significant improvement over the default model, suggesting that the default parameters are already optimal for this dataset.")

# Feature importance
print("\nMost Important Features in Predicting Heart Disease:")

important_features = X.columns[indices][:5]

print(", ".join(important_features))

# Implications
print("\nImplications for Clinical Use:")

print("1. The identified important features (e.g., 'thalach', 'cp') are critical in predicting heart disease and can be targeted for closer monitoring in clinical practice.")

print("2. The improved accuracy with the tuned model indicates the potential for enhanced predictive performance with further optimization.")

print("3. These findings can assist clinicians in focusing on the most relevant features for early detection and management of heart disease.")

# Grid search for hyper parameter tuning
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

grid_search.fit(X_train, y_train)

print("Best parameters found: ", grid_search.best_params_)

## SHAP for model interpretation
import shap

# Fit the model
rf_tuned.fit(X_train, y_train)

# Explain the model's predictions using SHAP
explainer = shap.TreeExplainer(rf_tuned)

shap_values = explainer.shap_values(X_test)

# Ensure the shap_values have the correct shape
if len(shap_values) == 2: # For binary classification, shap_values returns a list of elements
    shap_values = shap_values[1] # Use shap_values for the positive class

# Plot the SHAP summary plot
shap.summary_plot(shap_values, X_test)